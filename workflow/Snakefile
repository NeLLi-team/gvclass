__version__ = "1.0.0"
"""fschulz@lbl.gov"""

from pathlib import Path
import glob
import subprocess
import datetime
import json
import os

from snakemake.io import load_configfile


""" Assign taxonomy to NCDLV contigs or MAGs """

"""
Input is single file with contig or collection of contigs
File ending ".fna" (nucleic acid) or ".faa" (amino acid)
No special characters in filename, no additional "."
Recommended assembly size is 50kb

Example run:
snakemake -j 8 --use-conda --config querydir="example"
snakemake -j 8 --use-conda --config querydir="<path to dir with query fna or faa>" database_path="<path to db>"
"""

def log_run_info(log_file):
    with open(log_file, "w") as f:
        f.write(f"GVClass Pipeline Version: {__version__}\n")
        f.write(f"Run Date and Time: {datetime.datetime.now()}\n\n")
        f.write("Configuration:\n")
        
        # Create a copy of the config to modify
        config_to_log = config.copy()
        
        # Use the actual database_path
        config_to_log['database_path'] = str(database_path)
        
        json.dump(config_to_log, f, indent=2)
        f.write("\n")

# Get the directory of the Snakefile
workflow_dir = Path(workflow.basedir).resolve()
configfile: workflow_dir / "config.yml"

# input and output dirs
querydir = Path(config["querydir"]).resolve()
outdir = querydir / "results"
database_path = Path(config.get("database_path", workflow_dir / "resources")).resolve()
conda_env = workflow_dir / "envs/gvclass.yml"

# query file bases = query names without suffix
QUERYBASENAMES = [x.stem for x in querydir.iterdir() if x.is_file() and x.suffix in [".faa", ".fna"]]

# Check and download resources if needed
def check_and_download_resources(database_path):
    database_path = Path(database_path).resolve()
    required_files = [
        "models/combined.hmm",
        "ncldvApril24_labels.txt",
        "models_APRIL24--databaseApril24.cutoffs",
        "order_completeness.tab"
    ]

    if not database_path.exists():
        print(f"Resources directory not found at {database_path}. Creating and downloading resources...")
        database_path.mkdir(parents=True, exist_ok=True)
        os.chdir(str(database_path))
        url = "https://portal.nersc.gov/cfs/nelli/gvclassDB/resources.tar.gz"
        subprocess.run(["wget", url], check=True)
        subprocess.run(["tar", "-xzvf", "resources.tar.gz", "--strip-components=1"], check=True)
        subprocess.run(["rm", "resources.tar.gz"], check=True)
        print(f"Resources downloaded and extracted to {database_path}.")
    else:
        print(f"Resources directory found at {database_path}.")

    missing_files = [f for f in required_files if not (database_path / f).exists()]
    if missing_files:
        raise ValueError(f"The following required files are missing from {database_path}:\n" + "\n".join(missing_files))

    return database_path


database_path = check_and_download_resources(database_path)

# model names
modelnames = [line.split()[1] for line in open(database_path / "models/combined.hmm") if "NAME" in line]
if config["mode_fast"] == True:
    modelnames = [x for x in modelnames if not x.startswith("OG")]

# labels file for tax assignments
labels = database_path / "ncldvApril24_labels.txt"

print(f"querydir: {querydir}")
print(f"outdir: {outdir}")
print(f"database_path: {database_path}")
print(f"QUERYBASENAMES: {QUERYBASENAMES}")

rule all:
    input:
        outdir / "run_info.log",
        expand(str(outdir / "{querybase}.tar.gz"), querybase=QUERYBASENAMES),
        str(outdir / f"gvclass_out_v{__version__}.tab")

""" 
Step 0 
Genecalling to find optimal translation table 
"""

rule log_info:
    output:
        log_file = outdir / "run_info.log"
    run:
        log_run_info(output.log_file)


rule reformat_faa:
    conda:
        str(conda_env)
    input:
        querydir / "{querybase}.faa",
        log_file = outdir / "run_info.log"
    log:
        outdir / "{querybase}/log/reformat/{querybase}.log"
    wildcard_constraints:
        querybase = "|".join(QUERYBASENAMES)
    output:
        outdir / "{querybase}/query_faa/{querybase}.faa",
        outdir / "{querybase}/stats/{querybase}.stats.tab"
    params:
        script = workflow_dir / "scripts/reformat.py"
    shell:
        """
        python {params.script} -i {input[0]} -o {output[0]} -s {output[1]} &> {log}
        """

rule genecalling:
    conda:
        str(conda_env)
    input:
        queryfna = querydir / "{querybase}.fna",
        models = database_path / "models/combined.hmm",
        log_file = outdir / "run_info.log"
    output:
        gffout = outdir / "{querybase}/query_gff/{querybase}.gff",
        genecalling_statsout = outdir / "{querybase}/stats/{querybase}.genecalling.tab",
        best_statsout = outdir / "{querybase}/stats/{querybase}.stats.tab",
        faafinalout = outdir / "{querybase}/query_faa/{querybase}.faa",
        fnafinalout = outdir / "{querybase}/query_fna/{querybase}.fna"
    params:
        modelsout = lambda wildcards: outdir / wildcards.querybase / "hmmout/models.out",
        script = workflow_dir / "scripts/opgecall.py"
    log:
        outdir / "{querybase}/log/genecalling/{querybase}.log"
    shell:
        """
        if [ -s {input.queryfna} ]; then
            python {params.script} -f {input.queryfna} -g {output.gffout} \
            -gs {output.genecalling_statsout} -ss {output.best_statsout} -fa {output.faafinalout} \
            -m {input.models} -fn {output.fnafinalout} -o {params.modelsout} &> {log}
        else
            echo "Input file {input.queryfna} is not available. Skipping gene calling." > {log}
            touch {output.gffout} {output.genecalling_statsout} {output.best_statsout} \
                  {output.fnafinalout} {params.modelsout}
        fi
        """
"""
Step 1 
Identify markers, extract, align
"""

rule run_hmmsearch:
    conda:
        str(conda_env)
    input:
        queryfaa = outdir / "{querybase}/query_faa/{querybase}.faa",
        models = database_path / "models/combined.hmm",
        cutoffs = database_path / "models_APRIL24--databaseApril24.cutoffs"
    output:
        hitcounts = outdir / "{querybase}/hmmout/models.counts",
        scoreout = outdir / "{querybase}/hmmout/models.score",
        modelsout = outdir / "{querybase}/hmmout/models.out",
        modelsout_f = outdir / "{querybase}/hmmout/models.out.filtered"
    log:
        outdir / "{querybase}/log/hmmsearch/{querybase}.log"
    wildcard_constraints:
        querybase = "|".join(QUERYBASENAMES)
    params:
        script = workflow_dir / "scripts/hmmsearch.py"
    shell:
        """
        if [ -s {input.queryfaa} ]; then
            python {params.script} -q {input.queryfaa} -m {input.models} \
              -h {output.modelsout} -hf {output.modelsout_f} \
              -c {output.hitcounts} -s {output.scoreout} -f {input.cutoffs} &> {log}
        else
            echo "Input file {input.queryfaa} is empty. Skipping hmmsearch." > {log}
            touch {output.modelsout} {output.hitcounts} {output.scoreout}
        fi
        """

rule extract_qhits:
    """
    extract hits from hmmsearch, one file per marker, merge with refs
    """
    conda:
        str(conda_env)
    input:
        hmmout = outdir / "{querybase}/hmmout/models.out.filtered",
        queryfaa = outdir / "{querybase}/query_faa/{querybase}.faa"
    output:
        queryhitsfaa = temp(outdir / "{querybase}/query_hits_faa/{modelbase}.faa") if not config["keep_temp"] else outdir / "{querybase}/query_hits_faa/{modelbase}.faa",
    params:
        script = workflow_dir / "scripts/extract_qhits.py"
    shell:
        """
        if [ -s {input.hmmout} ]; then
            python {params.script} -h {input.hmmout} -q {input.queryfaa} \
            -o {output.queryhitsfaa}
        else
            echo "No hits found in hmmsearch output. Skipping hit extraction."
            touch {output.queryhitsfaa}
        fi
        """

rule blastp_reduce_merge:
    """
    blastp vs databases of representative genomes
    extract up to top 100 hits
    """
    conda:
        str(conda_env)
    input:
        queryhitsfaa = outdir / "{querybase}/query_hits_faa/{modelbase}.faa",
        reffaa = database_path / "database/faa/{modelbase}.faa",
        refdb = database_path / "database/dmnd/{modelbase}.dmnd"
    output:
        blastpout = temp(outdir / "{querybase}/blastp_out/{modelbase}.m8") if not config["keep_temp"] else outdir / "{querybase}/blastp_out/{modelbase}.m8",
        mergedfaa = temp(outdir / "{querybase}/query_hits_merged_faa/{modelbase}.faa") if not config["keep_temp"] else outdir / "{querybase}/query_hits_merged_faa/{modelbase}.faa"
    wildcard_constraints:
        querybase = "|".join(QUERYBASENAMES)
    params:
        script = workflow_dir / "scripts/blastp_reduce_merge.py"
    shell:
        """
        python {params.script} -q {input.queryhitsfaa} -r {input.reffaa} \
        -d {input.refdb} -b {output.blastpout} -o {output.mergedfaa}
        touch {output.blastpout} {output.mergedfaa}
        """

rule align_trim:
    """
    align extracted proteins together with refs
    """
    conda:
        str(conda_env)
    input:
        mergedfaa = outdir / "{querybase}/query_hits_merged_faa/{modelbase}.faa"
    output:
        aln = temp(outdir / "{querybase}/queryrefs_aligned/{modelbase}.mafft") if not config["keep_temp"] else outdir / "{querybase}/queryrefs_aligned/{modelbase}.mafft",
        trimmedaln = temp(outdir / "{querybase}/queryrefs_aligned/{modelbase}.mafft01") if not config["keep_temp"] else outdir / "{querybase}/queryrefs_aligned/{modelbase}.mafft01"
    params:
        script = workflow_dir / "scripts/align_trim.py"
    shell:
        """
        python {params.script} -q {input.mergedfaa} -a {output.aln} \
        -t {output.trimmedaln} -o {config[mafftoption]}
        touch {output.aln} {output.trimmedaln}
        """

"""
Step 2 - Tree based
Build protein trees, get nearest neighbor in trees
"""

rule build_trees:
    """
    build single protein trees for models
    no log to reduce temp files
    """
    conda:
        str(conda_env)
    input:
        trimmedaln = outdir / "{querybase}/queryrefs_aligned/{modelbase}.mafft01"
    output:
        tree = temp(outdir / "{querybase}/queryrefs_genetrees/{modelbase}.treefile") if not config["keep_temp"] else outdir / "{querybase}/queryrefs_genetrees/{modelbase}.treefile"
    params:
        script = workflow_dir / "scripts/build_tree.py"
    shell:
        """
        python {params.script} -a {input.trimmedaln} -t {output.tree} \
        -m {config[treeoption]}
        touch {output.tree}
        """

rule get_nn:
    """
    infer nearest neighbor in each tree
    """
    conda:
        str(conda_env)
    params:
        queryname="{querybase}",
        fdir = lambda wildcards: outdir / wildcards.querybase,
        script = workflow_dir / "scripts/get_nn_tree.py"
    input:
        ft = lambda wildcards: expand(outdir / wildcards.querybase / "queryrefs_genetrees/{modelbase}.treefile", modelbase=modelnames),
        alnt = lambda wildcards: expand(outdir / wildcards.querybase / "queryrefs_aligned/{modelbase}.mafft01", modelbase=modelnames)
    log:
        outdir / "{querybase}/log/nn/{querybase}.log"
    wildcard_constraints:
        querybase = "|".join(QUERYBASENAMES)
    output:
        tree_out = outdir / "{querybase}/stats/{querybase}.tree_nn"
    shell: 
        """
        python {params.script} -q {params.queryname} -t {params.fdir}/queryrefs_genetrees/ \
        -o {output.tree_out} -l {labels} &> {log}
        """

rule summarize:
    """
    combine different outputs
    """
    conda:
        str(conda_env)
    input:
        nn_tree = outdir / "{querybase}/stats/{querybase}.tree_nn",
        models_count = outdir / "{querybase}/hmmout/models.counts",
        querystats = outdir / "{querybase}/stats/{querybase}.stats.tab"
    log:
        outdir / "{querybase}/log/summarize/{querybase}.log"
    params:
        conversiontable = database_path / "order_completeness.tab",
        script = workflow_dir / "scripts/summarize.py"
    output:
        outdir / "{querybase}/{querybase}.summary.tab"
    wildcard_constraints:
        querybase = "|".join(QUERYBASENAMES)
    shell:
        """
        echo "Executing summarize rule for {wildcards.querybase}" >> {log}
        echo "Input files:" >> {log}
        echo "  nn_tree: {input.nn_tree}" >> {log}
        echo "  models_count: {input.models_count}" >> {log}
        echo "  querystats: {input.querystats}" >> {log}
        echo "Output file: {output}" >> {log}
        echo "Conversion table: {params.conversiontable}" >> {log}
        
        python {params.script} -n {input.nn_tree} -g {input.models_count} \
          -q {input.querystats} -s {output}  -f {params.conversiontable}   &>> {log}
        
        echo "Summarize rule completed" >> {log}
        """

rule combinedout:
    conda:
        str(conda_env)
    input:
        expand(outdir / "{querybase}/{querybase}.summary.tab", querybase=QUERYBASENAMES)
    params:
        fdirp = outdir,
        script = workflow_dir / "scripts/combinedout.py",
        version = __version__
    output:
        combined = outdir / f"gvclass_out_v{__version__}.tab"
    shell:
        """
        python {params.script} -r {params.fdirp} -o {output.combined} -v {params.version}
        touch {output.combined}
        """

rule cleanup:
    conda:
        str(conda_env)
    input:
        summary = outdir / "{querybase}/{querybase}.summary.tab",
        combined = str(outdir / f"gvclass_out_v{__version__}.tab")
    params:
        fdir = lambda wildcards: outdir / wildcards.querybase,
        faaout = lambda wildcards: outdir / wildcards.querybase / "query_faa" / f"{wildcards.querybase}.faa",
        faaout_copy = lambda wildcards: outdir / f"{wildcards.querybase}.faa",
    output:
        outdir / "{querybase}.tar.gz"
    wildcard_constraints:
        querybase = "|".join(QUERYBASENAMES)
    shell:
        """
        if [ -f {params.faaout} ]; then
            cp {params.faaout} {params.faaout_copy}
        else
            echo "Warning: {params.faaout} not found. Skipping copy."
        fi
        # Remove empty dirs
        find {params.fdir} -type f -size 0 -delete
        find {params.fdir} -type d -empty -delete
        # Ensure the output directory exists before trying to write the .tar.gz file
        mkdir -p $(dirname {output})
        # Change to the parent directory of the target directory to keep directory structure in the tar file
        cd $(dirname {params.fdir})
        # Create the .tar.gz file
        tar -zcvf $(basename {output}) $(basename {params.fdir})
        cd -
        # Remove the original directory after successful compression
        rm -rf {params.fdir}
        """