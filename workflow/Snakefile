from pathlib import Path
import glob
import subprocess

""" Assign taxonomy to NCDLV contigs or MAGs """

"""
Input is single file with contig or collection of contigs
File ending ".fna" (nucleic acid) or ".faa" (amino acid)
No special characters in filename, no additional "."
Recommended assembly size is 50kb

Example run:
snakemake -j 8 --use-conda --config querydir="example"
"""

configfile: "workflow/config.yml"

# input and output dirs
querydir = Path(config["querydir"])
outdir = querydir / "results"

# Default database path
database_path = Path(config.get("database_path", Path.cwd() / "resources"))
database_path = database_path.resolve()

# query file bases = query names without suffix
QUERYBASENAMES = [x.stem for x in querydir.iterdir() if x.is_file() and x.suffix in [".faa", ".fna"]]

# Check and download resources if needed
def check_and_download_resources(database_path):
    if not database_path.exists():
        print(f"Resources directory not found at {database_path}. Downloading...")
        url = "https://portal.nersc.gov/cfs/nelli/gvclassDB/resources.tar.gz"
        subprocess.run(["wget", url], check=True)
        subprocess.run(["tar", "-xzvf", "resources.tar.gz"], check=True)
        subprocess.run(["rm", "resources.tar.gz"], check=True)
        print("Resources downloaded and extracted.")
    else:
        print("Resources directory already exists.")

check_and_download_resources(database_path)

# model names
modelnames = [line.split()[1] for line in open(database_path / "models/combined.hmm") if "NAME" in line]
if config["mode_fast"] == True:
    modelnames = [x for x in modelnames if not x.startswith("OG")]

# labels file for tax assignments
labels = database_path / "ncldvApril24_labels.txt"

rule all:
    input:
        expand(str(outdir / "{querybase}.tar.gz"), querybase=QUERYBASENAMES),
        str(outdir / "gvclass_out.tab"),
        expand(str(outdir / "{querybase}.tar.gz"), querybase=QUERYBASENAMES)

""" 
Step 0 
Genecalling to find optimal translation table 
"""

rule reformat_faa:
    """
    check format of faa input and reformat
    reformat headers to ><filenamebase>|<proteinid> 
    create summary stats, asssembly size, GC, coding density, gene count
    if faa provided add gene count only
    """
    conda:
        "envs/gvclass.yml"
    input:
        querydir / "{querybase}.faa"
    log:
        outdir / "{querybase}/log/reformat/{querybase}.log"
    output:
        outdir / "{querybase}/query_faa/{querybase}.faa",
        outdir / "{querybase}/stats/{querybase}.stats.tab"
    shell:
        """
        python workflow/scripts/reformat.py -i {input} -o {output[0]} -s {output[1]} &> {log}
        """

rule genecalling:
    """
    Genecalling with different translation tables
    Find code that optimizes coding density
    """
    conda:
        "envs/gvclass.yml"
    input:
        queryfna = querydir / "{querybase}.fna",
        models = database_path / "models/combined.hmm"
    log:
        outdir / "{querybase}/log/genecalling/{querybase}.log"
    output:
        gffout = outdir / "{querybase}/query_gff/{querybase}.gff",
        genecalling_statsout = outdir / "{querybase}/stats/{querybase}.genecalling.tab",
        best_statsout = outdir / "{querybase}/stats/{querybase}.stats.tab",
        faafinalout = outdir / "{querybase}/query_faa/{querybase}.faa",
        fnafinalout = outdir / "{querybase}/query_fna/{querybase}.fna"
    params:
        modelsout = outdir / "{querybase}/hmmout/models.out"
    shell:
        """
        if [ -s {input.queryfna} ]; then
            python workflow/scripts/opgecall.py -f {input.queryfna} -g {output.gffout} \
            -gs {output.genecalling_statsout} -ss {output.best_statsout} -fa {output.faafinalout} -m {input.models} -fn {output.fnafinalout} -o {params.modelsout} &> {log}
        else
            echo "Input file {input.queryfna} is not available. Skipping gene calling." > {log}
            touch {output.gffout} {output.genecalling_statsout} {output.best_statsout} {output.fnafinalout} {params.modelsout}
        fi
        """

"""
Step 1 
Identify markers, extract, align
"""

rule run_hmmsearch:
    """
    hmmsearch to identify marker genes in query sequences
    """
    conda:
        "envs/gvclass.yml"
    input:
        queryfaa = outdir / "{querybase}/query_faa/{querybase}.faa",
        models = database_path / "models/combined.hmm",
        cutoffs = database_path / "models_APRIL24--databaseApril24.cutoffs"
    log:
        outdir / "{querybase}/log/hmmsearch/{querybase}.log"
    output:
        hitcounts = outdir / "{querybase}/hmmout/models.counts",
        scoreout = outdir / "{querybase}/hmmout/models.score",
        modelsout = outdir / "{querybase}/hmmout/models.out",
        modelsout_f = outdir / "{querybase}/hmmout/models.out.filtered",
    shell:
        """
        if [ -s {input.queryfaa} ]; then
            python workflow/scripts/hmmsearch.py -q {input.queryfaa} -m {input.models} \
              -h {output.modelsout} -hf {output.modelsout_f} \
              -c {output.hitcounts} -s {output.scoreout} -f {input.cutoffs} &> {log}
        else
            echo "Input file {input.queryfaa} is empty. Skipping hmmsearch." > {log}
            touch {output.modelsout} {output.hitcounts} {output.scoreout}
        fi
        """

rule extract_qhits:
    """
    extract hits from hmmsearch, one file per marker, merge with refs
    """
    conda:
        "envs/gvclass.yml"
    input:
        hmmout = outdir / "{querybase}/hmmout/models.out.filtered",
        queryfaa = outdir / "{querybase}/query_faa/{querybase}.faa"
    output:
        queryhitsfaa = temp(outdir / "{querybase}/query_hits_faa/{modelbase}.faa") if not config["keep_temp"] else outdir / "{querybase}/query_hits_faa/{modelbase}.faa",
    shell:
        """
        if [ -s {input.hmmout} ]; then
            python workflow/scripts/extract_qhits.py -h {input.hmmout} -q {input.queryfaa} \
            -o {output.queryhitsfaa}
        else
            echo "No hits found in hmmsearch output. Skipping hit extraction."
            touch {output.queryhitsfaa}
        fi
        """

rule blastp_reduce_merge:
    """
    blastp vs databases of representative genomes
    extract up to top 100 hits
    """
    conda:
        "envs/gvclass.yml"
    input:
        queryhitsfaa = outdir / "{querybase}/query_hits_faa/{modelbase}.faa",
        reffaa = database_path / "database/faa/{modelbase}.faa",
        refdb = database_path / "database/dmnd/{modelbase}.dmnd"
    output:
        blastpout = temp(outdir / "{querybase}/blastp_out/{modelbase}.m8") if not config["keep_temp"] else outdir / "{querybase}/blastp_out/{modelbase}.m8",
        mergedfaa = temp(outdir / "{querybase}/query_hits_merged_faa/{modelbase}.faa") if not config["keep_temp"] else outdir / "{querybase}/query_hits_merged_faa/{modelbase}.faa"
    shell:
        """
        python workflow/scripts/blastp_reduce_merge.py -q {input.queryhitsfaa} -r {input.reffaa} \
        -d {input.refdb} -b {output.blastpout} -o {output.mergedfaa}
        touch {output.blastpout} {output.mergedfaa}
        """

rule align_trim:
    """
    align extracted proteins together with refs
    """
    conda:
        "envs/gvclass.yml"
    input:
        mergedfaa = outdir / "{querybase}/query_hits_merged_faa/{modelbase}.faa"
    output:
        aln = temp(outdir / "{querybase}/queryrefs_aligned/{modelbase}.mafft") if not config["keep_temp"] else outdir / "{querybase}/queryrefs_aligned/{modelbase}.mafft",
        trimmedaln = temp(outdir / "{querybase}/queryrefs_aligned/{modelbase}.mafft01") if not config["keep_temp"] else outdir / "{querybase}/queryrefs_aligned/{modelbase}.mafft01"
    shell:
        """
        python workflow/scripts/align_trim.py -q {input.mergedfaa} -a {output.aln} \
        -t {output.trimmedaln} -o {config[mafftoption]}
        touch {output.aln} {output.trimmedaln}
        """

"""
Step 2 - Tree based
Build protein trees, get nearest neighbor in trees
"""

rule build_trees:
    """
    build single protein trees for models
    no log to reduce temp files
    """
    conda:
        "envs/gvclass.yml"
    input:
        trimmedaln = outdir / "{querybase}/queryrefs_aligned/{modelbase}.mafft01"
    output:
        tree = temp(outdir / "{querybase}/queryrefs_genetrees/{modelbase}.treefile") if not config["keep_temp"] else outdir / "{querybase}/queryrefs_genetrees/{modelbase}.treefile"
    shell:
        """
        python workflow/scripts/build_tree.py -a {input.trimmedaln} -t {output.tree} \
        -m {config[treeoption]}
        touch {output.tree}
        """

rule get_nn:
    """
    infer nearest neighbor in each tree
    """
    conda:
        "envs/gvclass.yml"
    params:
        queryname="{querybase}",
        fdir = outdir / "{querybase}"
    input:
        ft = expand(outdir / "{{querybase}}/queryrefs_genetrees/{modelbase}.treefile", modelbase=modelnames),
        alnt = expand(outdir / "{{querybase}}/queryrefs_aligned/{modelbase}.mafft01", modelbase=modelnames)
    log:
        outdir / "{querybase}/log/nn/{querybase}.log"
    output:
        tree_out = outdir / "{querybase}/stats/{querybase}.tree_nn",
    shell: 
        """
        python workflow/scripts/get_nn_tree.py -q {params.queryname} -t {params.fdir}/queryrefs_genetrees/ \
        -o {output.tree_out} -l {labels} &> {log}
        """

rule summarize:
    """
    combine different outputs
    """
    conda:
        "envs/gvclass.yml"
    input:
        nn_tree = outdir / "{querybase}/stats/{querybase}.tree_nn",
        models_count = outdir / "{querybase}/hmmout/models.counts",
        querystats = outdir / "{querybase}/stats/{querybase}.stats.tab"
    log:
        outdir / "{querybase}/log/summarize/{querybase}.log"
    params:
        conversiontable = database_path / "order_completeness.tab"
    output:
        outdir / "{querybase}/{querybase}.summary.tab"
    shell:
        """
        python workflow/scripts/summarize.py -n {input.nn_tree} -g {input.models_count} \
          -q {input.querystats} -s {output}  -f {params.conversiontable}   &> {log}
        """

rule combinedout:
    """
    combined output for multiple query files
    """
    conda:
        "envs/gvclass.yml"
    input:
        expand(outdir / "{querybase}/{querybase}.summary.tab", querybase=QUERYBASENAMES)
    params:
        fdirp = outdir
    output:
        combined = outdir / "gvclass_out.tab"
    shell:
        """
        python workflow/scripts/combinedout.py -r {params.fdirp} -o {output.combined}
        touch {output.combined}
        """

rule cleanup:
    """
    intermediate files are kept including empty files, compress output 
    """
    conda:
        "envs/gvclass.yml"
    input:
        summary = outdir / "{querybase}/{querybase}.summary.tab",
        combined = outdir / "gvclass_out.tab"
    params:
        fdir = outdir / "{querybase}",
        faaout = outdir / "{querybase}/query_faa/{querybase}.faa",
        faaout_copy = outdir / "{querybase}.faa"
    output:
        outdir / "{querybase}.tar.gz"
    shell:
        """
        if [ -f {params.faaout} ]; then
            cp {params.faaout} {params.faaout_copy}
        else
            echo "Warning: {params.faaout} not found. Skipping copy."
        fi
        # Remove empty dirs
        find {params.fdir} -type f -size 0 -delete
        find {params.fdir} -type d -empty -delete
        # Ensure the output directory exists before trying to write the .tar.gz file
        mkdir -p $(dirname {output})
        # Change to the parent directory of the target directory to keep directory structure in the tar file
        cd $(dirname {params.fdir})
        # Create the .tar.gz file
        tar -zcvf $(basename {output}) $(basename {params.fdir})
        cd -
        # Remove the original directory after successful compression
        rm -rf {params.fdir}
        """