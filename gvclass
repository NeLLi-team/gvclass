#!/usr/bin/env python
"""
Run GVClass pipeline with clean output and resource monitoring.
"""

import sys
import os
import csv
from pathlib import Path

def resolve_repo_dir(script_path: Path) -> Path:
    """Resolve the GVClass repo directory when running from a copied script."""
    candidate = script_path.parent
    if (candidate / "src").exists():
        return candidate

    env_path = os.environ.get("GVCLASS_REPO")
    if env_path:
        env_repo = Path(env_path).expanduser().resolve()
        if (env_repo / "src").exists():
            return env_repo

    repo_path_file = Path.home() / ".config" / "gvclass" / "repo_path"
    if repo_path_file.exists():
        repo_path = repo_path_file.read_text().strip()
        if repo_path:
            file_repo = Path(repo_path).expanduser().resolve()
            if (file_repo / "src").exists():
                return file_repo

    cwd = Path.cwd().resolve()
    for parent in [cwd] + list(cwd.parents):
        if (parent / "src").exists() and (parent / "gvclass").exists():
            return parent

    return candidate


# CRITICAL: Find the repo directory even when symlinked/copied
script_path = Path(__file__).resolve()  # Follows symlinks
repo_dir = resolve_repo_dir(script_path)

# Add repo to Python path so we can import src.*
if str(repo_dir) not in sys.path:
    sys.path.insert(0, str(repo_dir))

import subprocess
import argparse
import re
import shutil
from datetime import datetime
import threading
import time


def count_files_in_directory(directory):
    """Count .fna and .faa files in a directory."""
    path = Path(directory)
    if not path.exists():
        return 0
    fna_files = list(path.glob("*.fna"))
    faa_files = list(path.glob("*.faa"))
    return len(fna_files) + len(faa_files)


def load_config(config_file="config/gvclass_config.yaml"):
    """Load configuration from YAML file."""
    script_dir = repo_dir
    config_path = Path(config_file)
    default_config = {
        "database": {
            "path": str(script_dir / "resources"),
            "download_url": "https://portal.nersc.gov/cfs/m342/GVClass/gvclass_db_v1.1.tar.gz",
            "expected_size": 701,
        },
        "pipeline": {
            "tree_method": "fasttree",
            "mode_fast": True,
            "threads": 16,
            "output_pattern": "{query_dir}_results",
        },
    }

    # If not absolute, try different locations for config file
    if not config_path.is_absolute():
        # Try in current directory first (project-specific)
        config_path = Path.cwd() / config_file

        if not config_path.exists():
            # Try old location in current directory
            config_path = Path.cwd() / "gvclass_config.yaml"

        if not config_path.exists():
            # Fall back to repo config
            config_path = script_dir / config_file

    if config_path.exists():
        try:
            import yaml
        except ModuleNotFoundError:
            print(
                "Warning: PyYAML is not available; using built-in defaults "
                f"instead of {config_path}.",
                file=sys.stderr,
            )
            return default_config
        with open(config_path, "r") as f:
            return yaml.safe_load(f)

    # Return default config with absolute path to resources
    return default_config


def check_and_setup_database(db_path, config):
    """Check if database exists, download if needed."""

    db_path = Path(db_path)

    # Check for key database files
    required_checks = [
        db_path / "models" / "combined.hmm",
        db_path / "database",
        db_path / "gvclassSeptember25_labels.tsv",
    ]

    if db_path.exists() and any(check.exists() for check in required_checks):
        print(f"‚úÖ Database found at: {db_path}")
        return True

    print(f"\nüì¶ Database not found at {db_path}")
    print("üì• Setting up database...")

    try:
        # Use DatabaseManager to setup the database
        from src.utils.database_manager import DatabaseManager

        DatabaseManager.setup_database(str(db_path))
        print("‚úÖ Database setup complete!")
        return True

    except Exception as e:
        print(f"\n‚ùå Failed to setup database: {e}")
        print("   You can try manually with: pixi run setup-db")
        return False


class ResourceMonitor:
    """Monitor progress and resource usage."""

    def __init__(self, output_dir, total_queries, process_pid, initial_completed=0):
        try:
            import psutil
        except ModuleNotFoundError as exc:
            print(
                "Warning: psutil is not available; resource monitoring is disabled. "
                "Install it (e.g., `pixi install` or `pip install psutil`) for memory stats.",
                file=sys.stderr,
            )
            self.psutil = None
        else:
            self.psutil = psutil
        self.output_dir = Path(output_dir)
        self.total_queries = total_queries
        self.process_pid = process_pid
        self.current_query = 0
        self.initial_completed = initial_completed  # For resume mode
        self.current_task = "Starting"
        self.running = True
        self.peak_memory_mb = 0
        self.current_memory_mb = 0

    def monitor(self):
        """Monitor progress and resources."""
        last_output = ""
        if self.psutil is None:
            process = None
        else:
            try:
                process = self.psutil.Process(self.process_pid)
            except (self.psutil.NoSuchProcess, self.psutil.AccessDenied):
                process = None

        while self.running:
            # Count completed queries by looking for summary.tab files in output root
            if self.output_dir.exists():
                total_completed = len(list(self.output_dir.glob("*.summary.tab")))
                # Subtract initial completed to get queries completed in this run
                self.current_query = total_completed - self.initial_completed

            # Monitor memory usage
            if process:
                try:
                    mem_info = process.memory_info()
                    self.current_memory_mb = mem_info.rss / 1024 / 1024
                    self.peak_memory_mb = max(
                        self.peak_memory_mb, self.current_memory_mb
                    )

                    # Include children processes
                    for child in process.children(recursive=True):
                        try:
                            child_mem = child.memory_info().rss / 1024 / 1024
                            self.current_memory_mb += child_mem
                            self.peak_memory_mb = max(
                                self.peak_memory_mb, self.current_memory_mb
                            )
                        except (self.psutil.NoSuchProcess, self.psutil.AccessDenied):
                            pass
                except (self.psutil.NoSuchProcess, self.psutil.AccessDenied):
                    pass

            # Display progress
            if self.total_queries > 0:
                percent = int((self.current_query / self.total_queries) * 100)
                output = f"Progress: [{percent:3d}%] Query {self.current_query}/{self.total_queries} | Memory: {self.current_memory_mb:.0f}MB (peak: {self.peak_memory_mb:.0f}MB)"
                if output != last_output:
                    print(f"\r{output:<80}", end="", flush=True)
                    last_output = output

            time.sleep(1)

    def stop(self):
        """Stop monitoring."""
        self.running = False


def main():
    """Run the pipeline with clean output and resource monitoring."""
    from pathlib import Path

    parser = argparse.ArgumentParser(
        description="Run GVClass pipeline with clean output"
    )
    parser.add_argument(
        "query_dir", nargs="?", help="Query directory containing .fna or .faa files"
    )
    parser.add_argument(
        "-o", "--output-dir", help="Output directory (default: <query_dir>_results)"
    )
    parser.add_argument(
        "-c",
        "--config",
        default="config/gvclass_config.yaml",
        help="Configuration file (default: config/gvclass_config.yaml)",
    )
    parser.add_argument("-d", "--database", help="Database path (overrides config)")
    parser.add_argument(
        "-t", "--threads", type=int, help="Number of threads (overrides config)"
    )
    parser.add_argument(
        "-j", "--max-workers", type=int, help="Maximum parallel workers"
    )
    parser.add_argument("--threads-per-worker", type=int, help="Threads per worker")
    parser.add_argument(
        "--tree-method",
        choices=["fasttree", "iqtree"],
        help="Tree building method (overrides config)",
    )
    parser.add_argument(
        "--mode-fast", action="store_true", help="Enable fast mode (overrides config)"
    )
    parser.add_argument(
        "--no-mode-fast",
        action="store_true",
        help="Disable fast mode (overrides config)",
    )
    parser.add_argument(
        "--cluster-type",
        default="local",
        choices=["local", "slurm", "pbs", "sge"],
        help="Type of cluster to use",
    )
    parser.add_argument("--cluster-queue", help="Queue/partition for cluster jobs")
    parser.add_argument("--cluster-project", help="Project/account for cluster billing")
    parser.add_argument(
        "--cluster-walltime", default="04:00:00", help="Walltime for cluster jobs"
    )
    parser.add_argument("-v", "--verbose", action="store_true", help="Verbose output")
    parser.add_argument(
        "--version",
        action="store_true",
        help="Show version information and exit",
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help="Resume from previous run, skipping completed queries",
    )

    args = parser.parse_args()

    # Handle --version flag
    if args.version:
        from src.utils.database_manager import DatabaseManager

        print("GVClass Pipeline")
        print("  Software version: v1.1.1")

        # Load config to get database path
        config = load_config(args.config)
        script_dir = Path(__file__).resolve().parent
        db_path = Path(args.database if args.database else config["database"]["path"])
        # Make database path absolute if it's relative
        if not db_path.is_absolute():
            db_path = script_dir / db_path

        if db_path.exists():
            db_version = DatabaseManager.get_database_version(db_path)
            print(f"  Database version: {db_version}")
        else:
            print("  Database version: not installed")

        sys.exit(0)

    if not args.query_dir:
        parser.print_help()
        sys.exit(0)

    # Load configuration
    config = load_config(args.config)
    script_dir = repo_dir

    # Convert query_dir to absolute path (stays in user's context)
    query_dir_abs = Path(args.query_dir).resolve()

    # Apply configuration with command-line overrides
    database = args.database if args.database else config["database"]["path"]
    # Make database path absolute if it's relative
    database = Path(database)
    if not database.is_absolute():
        database = script_dir / database
    database = str(database)
    threads = args.threads if args.threads else config["pipeline"].get("threads", 16)
    tree_method = (
        args.tree_method
        if args.tree_method
        else config["pipeline"].get("tree_method", "fasttree")
    )

    # Handle mode_fast with explicit flags taking priority
    if args.no_mode_fast:
        mode_fast = False
    elif args.mode_fast:
        mode_fast = True
    else:
        mode_fast = config["pipeline"].get("mode_fast", True)

    # Handle output directory and make it absolute
    if args.output_dir:
        output_dir = str(Path(args.output_dir).resolve())
    else:
        query_base = query_dir_abs.name
        output_pattern = config["pipeline"].get("output_pattern", "{query_dir}_results")
        output_dir = str(Path(output_pattern.format(query_dir=query_base)).resolve())

    # Check and setup database
    if not check_and_setup_database(database, config):
        sys.exit(1)

    # Count input files
    n_queries = count_files_in_directory(query_dir_abs)
    n_skipped = 0

    if n_queries == 0:
        print(
            f"‚ùå ERROR: No .fna or .faa files found in {query_dir_abs}",
            file=sys.stderr,
        )
        sys.exit(1)

    # Create output directory if it doesn't exist (needed for resume check)
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # If resume mode, count already completed queries
    if args.resume:
        output_path = Path(output_dir)
        # Count queries that have both summary.tab and tar.gz
        # Need to remove the full extension for both file types
        summary_files = set(
            f.name.replace(".summary.tab", "")
            for f in output_path.glob("*.summary.tab")
        )
        tar_files = set(
            f.name.replace(".tar.gz", "") for f in output_path.glob("*.tar.gz")
        )
        n_skipped = len(summary_files & tar_files)  # Intersection

    # Calculate workers and threads per worker
    # If both -j and --threads-per-worker are specified, use them directly
    if args.max_workers and args.threads_per_worker:
        n_workers = args.max_workers
        threads_per_worker = args.threads_per_worker
    # If only -j is specified, use it with remaining threads distributed
    elif args.max_workers:
        n_workers = args.max_workers
        threads_per_worker = max(1, threads // n_workers)
    # If only --threads-per-worker is specified, calculate workers
    elif args.threads_per_worker:
        threads_per_worker = args.threads_per_worker
        n_workers = min(n_queries, max(1, threads // threads_per_worker))
    # If neither is specified, use heuristics
    else:
        if n_queries <= 4 and threads >= n_queries * 2:
            n_workers = n_queries
            threads_per_worker = threads // n_workers
        else:
            # Default to 4 threads per worker as a good balance
            threads_per_worker = 4
            n_workers = min(n_queries, max(1, threads // threads_per_worker))

    # Print clean header with ASCII art and color gradient
    print("\n")
    # ANSI color codes for gradient: purple -> pink -> red -> orange -> yellow
    colors = [
        "\033[95m",  # Purple (G)
        "\033[95m",  # Purple (V)
        "\033[91m",  # Pink (C)
        "\033[31m",  # Red (L)
        "\033[31m",  # Red (A)
        "\033[38;5;208m",  # Orange (S) - 256 color
        "\033[93m",  # Yellow (S)
    ]
    reset = "\033[0m"
    indent = "    "
    inner_width = 61

    def strip_ansi(text: str) -> str:
        return re.sub(r"\x1b\[[0-9;]*m", "", text)

    def frame_line(content: str = "") -> str:
        visible_len = len(strip_ansi(content))
        padding = " " * max(0, inner_width - visible_len)
        return f"{indent}‚ïë{content}{padding}‚ïë"

    print(f"{indent}‚ïî{'‚ïê' * inner_width}‚ïó")
    print(frame_line())
    print(
        frame_line(
            f"  {colors[0]}‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó{reset} {colors[1]}‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó{reset}{colors[2]} ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó{reset}{colors[3]}‚ñà‚ñà‚ïó{reset}      {colors[4]}‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó{reset} {colors[5]}‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó{reset}{colors[6]}‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó{reset}"
        )
    )
    print(
        frame_line(
            f" {colors[0]}‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù{reset} {colors[1]}‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë{reset}{colors[2]}‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù{reset}{colors[3]}‚ñà‚ñà‚ïë{reset}     {colors[4]}‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó{reset}{colors[5]}‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù{reset}{colors[6]}‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù{reset}"
        )
    )
    print(
        frame_line(
            f" {colors[0]}‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ñà‚ïó{reset}{colors[1]}‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë{reset}{colors[2]}‚ñà‚ñà‚ïë{reset}     {colors[3]}‚ñà‚ñà‚ïë{reset}     {colors[4]}‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë{reset}{colors[5]}‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó{reset}{colors[6]}‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó{reset}"
        )
    )
    print(
        frame_line(
            f" {colors[0]}‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë{reset}{colors[1]}‚ïö‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïù{reset}{colors[2]}‚ñà‚ñà‚ïë{reset}     {colors[3]}‚ñà‚ñà‚ïë{reset}     {colors[4]}‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë{reset}{colors[5]}‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë{reset}{colors[6]}‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë{reset}"
        )
    )
    print(
        frame_line(
            f" {colors[0]}‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù{reset} {colors[1]}‚ïö‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù{reset} {colors[2]}‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó{reset}{colors[3]}‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó{reset}{colors[4]}‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë{reset}{colors[5]}‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë{reset}{colors[6]}‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë{reset}"
        )
    )
    print(
        frame_line(
            f"  {colors[0]}‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{reset}   {colors[1]}‚ïö‚ïê‚ïê‚ïê‚ïù{reset}   {colors[2]}‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{reset}{colors[3]}‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{reset}{colors[4]}‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù{reset}{colors[5]}‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{reset}{colors[6]}‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù{reset}"
        )
    )
    print(frame_line())
    print(frame_line("         Giant Virus Classification Tool v1.1.1"))
    print(frame_line())
    print(f"{indent}‚ïö{'‚ïê' * inner_width}‚ïù")
    print()
    print("=" * 60)
    print("                    Pipeline Configuration")
    print("=" * 60)
    print(f"Config file: {args.config}")
    print(f"Query directory: {query_dir_abs}")
    print(f"Output directory: {output_dir}")
    print(f"Database: {database}")
    threads_used = n_workers * threads_per_worker
    print(
        f"Threads: {threads_used} used / {threads} requested "
        f"(Workers: {n_workers} √ó {threads_per_worker} threads)"
    )
    print(f"Tree method: {tree_method}")
    print(f"Fast mode: {mode_fast}")
    if args.resume:
        print("Resume mode: ENABLED")
        if n_skipped > 0:
            print(f"  - Skipping {n_skipped} completed queries")
            print(f"  - Processing {n_queries - n_skipped} remaining queries")
        else:
            print("  - No completed queries found")
    else:
        print("Resume mode: DISABLED")
    print(f"Total queries: {n_queries}")
    print("=" * 60)

    start_time = datetime.now()

    # Use pixi Python if available, otherwise fall back to current Python
    pixi_python = script_dir / ".pixi" / "envs" / "default" / "bin" / "python"
    python_exe = str(pixi_python) if pixi_python.exists() else sys.executable
    use_pixi_run = False
    pixi_cmd = shutil.which("pixi")
    if not pixi_python.exists() and pixi_cmd:
        use_pixi_run = True

    # Preflight dependency check for the subprocess environment
    def check_missing_deps() -> list:
        missing = []
        for dep in ["click"]:
            if use_pixi_run:
                result = subprocess.run(
                    [pixi_cmd, "run", "python", "-c", f"import {dep}"],
                    capture_output=True,
                    text=True,
                    cwd=script_dir,
                )
            else:
                result = subprocess.run(
                    [python_exe, "-c", f"import {dep}"],
                    capture_output=True,
                    text=True,
                )
            if result.returncode != 0:
                missing.append(dep)
        return missing

    missing_deps = check_missing_deps()
    if missing_deps and pixi_cmd:
        print("üì¶ Installing dependencies with pixi...")
        subprocess.run([pixi_cmd, "install"], cwd=script_dir)
        missing_deps = check_missing_deps()
    if missing_deps:
        print(
            "‚ùå Missing Python dependencies in the runtime environment: "
            + ", ".join(missing_deps)
        )
        print("   Run `pixi install` or use `pixi run gvclass ...`.")
        sys.exit(1)

    # Build command with absolute paths
    if use_pixi_run:
        cmd = [
            pixi_cmd,
            "run",
            "python",
            "-m",
            "src.bin.gvclass_prefect",
        ]
    else:
        cmd = [
            python_exe,
            "-m",
            "src.bin.gvclass_prefect",
        ]
    cmd += [
        str(query_dir_abs),
        "-o",
        output_dir,
        "-d",
        database,
        "-t",
        str(threads),
        "-j",
        str(n_workers),
        "--threads-per-worker",
        str(threads_per_worker),
        "--tree-method",
        tree_method,
        "--cluster-type",
        args.cluster_type,
    ]

    if mode_fast:
        cmd.append("--mode-fast")
    else:
        cmd.append("--no-mode-fast")

    if args.cluster_queue:
        cmd.extend(["--cluster-queue", args.cluster_queue])
    if args.cluster_project:
        cmd.extend(["--cluster-project", args.cluster_project])
    if args.cluster_walltime:
        cmd.extend(["--cluster-walltime", args.cluster_walltime])
    if args.verbose:
        cmd.append("--verbose")
    if args.resume:
        cmd.append("--resume")

    # Set environment
    env = os.environ.copy()
    existing_pythonpath = env.get("PYTHONPATH", "")
    repo_path_str = str(repo_dir)
    if existing_pythonpath:
        if repo_path_str not in existing_pythonpath.split(os.pathsep):
            env["PYTHONPATH"] = os.pathsep.join([repo_path_str, existing_pythonpath])
    else:
        env["PYTHONPATH"] = repo_path_str
    env["PREFECT_API_URL"] = ""
    env["PREFECT_LOGGING_LEVEL"] = "ERROR"
    env["PYTHONWARNINGS"] = "ignore"
    # Increase timeouts for cluster compatibility
    env["PREFECT_SERVER_STARTUP_TIMEOUT"] = "300"  # 5 minutes for slow cluster nodes
    env["PREFECT_API_DATABASE_TIMEOUT"] = "60"  # 1 minute for database operations

    # Start process (run in repo directory so it can import src.*)
    if args.verbose:
        process = subprocess.Popen(cmd, env=env, cwd=script_dir)
    else:
        process = subprocess.Popen(
            cmd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=script_dir
        )

    # Start resource monitor with actual queries to process
    queries_to_process = n_queries - n_skipped if args.resume else n_queries
    monitor = ResourceMonitor(
        output_dir, queries_to_process, process.pid, initial_completed=n_skipped
    )
    monitor_thread = threading.Thread(target=monitor.monitor)
    monitor_thread.start()

    try:
        # Wait for process and capture output if not verbose
        stdout_data, stderr_data = process.communicate()
        return_code = process.returncode

        # Stop monitor
        monitor.stop()
        monitor_thread.join()
        print("\r" + " " * 80 + "\r", end="")  # Clear line

        if return_code == 0:
            elapsed = datetime.now() - start_time
            minutes = int(elapsed.total_seconds() / 60)
            seconds = int(elapsed.total_seconds() % 60)

            print("\n‚úÖ Pipeline completed successfully!")

            # Validate outputs before generating summary
            print("üîç Validating pipeline outputs...")
            from src.utils.output_validator import validate_pipeline_outputs

            validation_results = validate_pipeline_outputs(
                output_dir=Path(output_dir), verbose=args.verbose
            )

            if not validation_results["success"]:
                print(
                    f"‚ö†Ô∏è  Validation found {len(validation_results['issues'])} issue(s):"
                )
                for issue in validation_results["issues"][:5]:  # Show first 5 issues
                    print(f"  - {issue}")
                if len(validation_results["issues"]) > 5:
                    print(f"  ... and {len(validation_results['issues']) - 5} more")
            else:
                print("‚úÖ All outputs validated successfully")

            # Generate combined summary from individual summary.tab files
            print("üìä Generating combined summary...")
            output_path = Path(output_dir)
            summary_files = list(output_path.glob("*.summary.tab"))

            if summary_files:
                # Read all individual summaries and combine them
                combined_rows = []
                headers = None

                for summary_file in sorted(summary_files):
                    with open(summary_file, "r", newline="") as f:
                        reader = csv.reader(f, delimiter="\t")
                        rows = list(reader)
                        if rows:
                            if headers is None and rows[0]:
                                headers = rows[0]
                            elif headers is not None and rows[0] != headers:
                                print(
                                    f"‚ö†Ô∏è  Header mismatch in {summary_file.name}; using first header"
                                )
                            for row in rows[1:]:
                                if row:
                                    combined_rows.append(row)

                # Write combined summary
                if combined_rows and headers:
                    combined_summary_tsv = output_path / "gvclass_summary.tsv"
                    combined_summary_csv = output_path / "gvclass_summary.csv"
                    with open(combined_summary_tsv, "w") as tsv_file, open(
                        combined_summary_csv, "w", newline=""
                    ) as csv_file:
                        csv_writer = csv.writer(csv_file)
                        tsv_file.write("\t".join(headers) + "\n")
                        csv_writer.writerow(headers)
                        for row in combined_rows:
                            tsv_file.write("\t".join(row) + "\n")
                            csv_writer.writerow(row)
                    print(
                        f"‚úÖ Combined summary written to: {combined_summary_tsv}"
                    )
                    print(
                        f"‚úÖ CSV summary written to: {combined_summary_csv}"
                    )
                else:
                    print("‚ö†Ô∏è  No summary data found to combine")
            else:
                print("‚ö†Ô∏è  No individual summary files found")

            print(f"‚è±Ô∏è  Runtime: {minutes}m {seconds}s")
            print(f"üíæ Peak memory usage: {monitor.peak_memory_mb:.0f} MB")
            print(f"üìä Results saved to: {output_dir}")

            # Final validation summary - removed since it's already handled above
        else:
            print(f"\n‚ùå Pipeline failed with exit code: {return_code}")
            if not args.verbose:
                if stderr_data:
                    try:
                        print(stderr_data.decode(errors="replace").strip())
                    except Exception:
                        print(stderr_data)
                if stdout_data:
                    try:
                        print(stdout_data.decode(errors="replace").strip())
                    except Exception:
                        print(stdout_data)
            sys.exit(1)

    except KeyboardInterrupt:
        monitor.stop()
        monitor_thread.join()
        print("\r" + " " * 80 + "\r", end="")
        print("\n‚ö†Ô∏è  Pipeline interrupted by user")
        process.terminate()
        sys.exit(1)
    except Exception as e:
        monitor.stop()
        monitor_thread.join()
        print("\r" + " " * 80 + "\r", end="")
        print(f"\n‚ùå Pipeline failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
